 # -*- coding: utf-8 -*-
"""
Created on Wed Nov 24 15:14:33 2021
Cameron Michie UNI ID: 10621521

This program checks and filters the data from the file names FILE1 and FILE2 and performs a least
squares fitting on the data to minimise values of z0 mass and z0 width. This occurs twice, once
with the partial width kept constant at the quoted value, and once where it is treated as a
variable and minimised alongside the mass and width. A plot is made of the data and is compared
to the predicted shape. The functions below with "_fpw" in the title have a fixed partial width,
whereas the ones without that in the title have variable partial width.

https://www.desmos.com/calculator/ths4xe1my0 This graph was helpful to me for plotting a).

Following that, the program creates a contour plot of the reduced chi-squared at variable z0 mass
and width. This process is quite computationally intense O(n^3), with a triple nested for loop
running for INTERVALS * INTERVALS * len(data) iterations. This is equally true for INTERVALS_2.

The code then produces a contour plot of the chi-squared zoomed in so that the contour lines
indicate standard deviations away from the minimised value. From this uncertanties on the
values of Z0 mass and Z0 width can be found, which hopefully agree with the ones found through
curve_fit.

The figure is saved to the default directory.

The code is versatile for other data sets, and other decay products, although the
contour plot assumes the width energy is smaller than the mass energy.

Last update 07/12/21

@author: Cameron Michie UNI ID: 10621521
"""

import time
import numpy as np
import matplotlib.pyplot as plt
from scipy.optimize import curve_fit
from matplotlib import ticker
import matplotlib.gridspec as gridspec


FILE1 = "C:/Users/camer/OneDrive/Documents/Intro to Programming/final assignment/z_boson_data_1.csv"
FILE2 = "C:/Users/camer/OneDrive/Documents/Intro to Programming/final assignment/z_boson_data_2.csv"
Z0_MASS_START = 100
Z0_WIDTH_START = 1
Z0_PARTIAL_WIDTH_START = 1
Z0_PARTIAL_WIDTH = 83.91e-03 # GeV
CONVERT_NATURAL_UNITS_TO_NB = 0.3894e+06 # nanobarns (nb)
OUTLIER_SIGMA = 5 # max allowed s.d. away from nearby datapoints before outlier rejected.

# the size of the square array used to calulated many chi-squared values of
# variable z0_mass and z0_width for the reduced chi-squared contour plot:
INTERVALS = 75      # For contour plot B) (wider range)
INTERVALS_2 = 50    # For contour plot C) (narrow range)

 # Increase for higher accuracy in the contour maps, decrease to lower the computation time.
 # WARNING: if programme runs but takes too long to finish running then lower this value.


def filecheck(file_name):
    """
    Checks FILE1 and FILE2 open without an IOError

    Parameters
    ----------
    file_name : str
        Location of file on computer.

    Returns
    -------
    True if opens.
    False if an IOError occurs.
    """

    try:
        file = open(file_name, 'r')
        file.close()
        return True

    except IOError:
        return False


def open_data(file_name):
    """
    Opens file_name and filters it.
    The filters reject and delete data entries for:
        any non-floats,
        an uncertainty of 0,
        any y values more than OUTLIER_SIGMA uncertainties away from
        neaby datapoints.

    Parameters
    ----------
    file_name : str
        Location of file on computer hard drive.

    Returns
    -------
    data : array
       returns array with form [x_values. y_values, y_error].

    """

    data = np.array([])
    data = np.genfromtxt(file_name, delimiter=',', comments='%', skip_header=1)

    for i in range(len(data)-1):

        if data[i, 2] in [0]:               #Turns 0s in y uncertainty into NaNs.
            data[i, 2] = np.nan

        # Turns outliers which are more than global variable OUTLIER_SIGMA standard deviations
        # away from nearby datapoints into NaNs.

        if np.absolute(data[i, 1] - data[i-1, 1]) > OUTLIER_SIGMA*data[i, 2] \
        and np.absolute(data[i, 1] - data[i+1, 1]) > OUTLIER_SIGMA*data[i, 2]:
            data[i, 1] = np.nan

    data = data[~np.isnan(data).any(axis=1)] # Removes all NaNs datapoints from the data.

    return data


def combine_data(firstfile, secondfile):
    """
    Uses open_data to open both files, stack them on top of each other
    and then sort them by ascending first column (energy or x values).

    Parameters
    ----------
    firstfile : string
        Location on computer of first datafile.
    secondfile : string
        Location on computer of second datafile.

    Returns
    -------
    combined_data : array
        Returns an array with form [x_values, y_values, y_error].

    """

    combined_data = np.array([])
    combined_data = np.vstack((open_data(firstfile), open_data(secondfile)))
    combined_data = combined_data[combined_data[:, 0].argsort()]
    return combined_data


def create_contour_data(data, x_swing, y_swing, intervals):
    """
    Contour data is a 2D array of z0 mass and width, which varies at fixed intervals between
    factors of x_swing/y_swing above and below the LSF minimised values for mass and width.
    For example, the function generates "intervals" numbers of x values between
    (1-x_swing) * LSF value < "intervals" numbers of x values < x_swing * LSF value.

    Parameters
    ----------
    data : array
        [x_values, y_values, ...].
    x_swing : float
        x values are generated between (1-x_swing) * LSF value < data < x_swing * LSF value
    y_swing : float
        y values are generated between (1-y_swing) * LSF value < data < y_swing * LSF value
    intervals : global variable
        Either input INTERVAL or INTERVAL_2
        The number of x or y data points generated, i.e the size of the square 2D array.

    Returns
    -------
    contour_data : array
        2D array of variable z0 mass z0 width.

    """

    contour_data = np.empty([0, 1])
    contour_data = np.array(np.linspace((1-x_swing)*curve_fitting(data)[0],\
                                        (1+x_swing)*curve_fitting(data)[0],\
                                        intervals))

    contour_data = np.vstack((contour_data,
                              np.linspace((1-y_swing)*curve_fitting(data)[1],\
                                            (1+y_swing)*curve_fitting(data)[1],\
                                            intervals)))
    return contour_data


def plot_data(data):
    """
    Plots a figure A) of crosssection against energy, for both the
    filtered experimental data and the predicted theoretical data with
    error bars.

    Plots also a contour map B) of the reduced chi-squared against variable
    z0 mass and z0 width.

    Saves plot to the default directory.

    Returns the contour's determiantion of the upper and lower bounds for Z0 mass and width,
    determined from the geometry of contour plot C).

    Parameters
    ----------
    data : array
        [x_values, y_values, ...].

    Returns
    -------
    upper and lower bounds of mass and width : tuple
        (z0_mass_upper_bound, z0_mass_lower_bound,
         z0_width_upper_bound, z0_width_lower_bound)

    """

    # Plot set-up
    fig = plt.figure(figsize=(7, 7))
    plt.style.use('ggplot')

    # Data set-up
    data = combine_data(FILE1, FILE2)
    contour_data_b = create_contour_data(data, 0.1, 0.333, INTERVALS)
    contour_data_c = create_contour_data(data, 0.005, 0.025, INTERVALS_2)


    # Plot a) Minimised crosssection


    # Titles and axes for plot a)
    ax_1 = fig.add_subplot(gridspec.GridSpec(3, 1)[0])
    ax_1.set_title(r'A) Crosssection, $\sigma $, for  $e^{-}e^{+} \longrightarrow$'\
                  r'$Z_{0}\longrightarrow  e^{-}e^{+}$')
    ax_1.set_xlabel('Energy (GeV)')
    ax_1.set_ylabel(r'$\sigma $ (nb)')
    # Plot lines on ax_1
    plt.plot(data[:, 0], data[:, 1],
             label="filtered data\nfrom experiment", color='b', alpha=0.75, linewidth=2.1)
    ax_1.errorbar(data[:, 0],\
              crosssection(data[:, 0], *curve_fitting(data)), \
                  yerr=data[:, 2], fmt='|', color='Crimson', alpha=0.5)
    plt.plot(data[:, 0],\
              crosssection(data[:, 0], *curve_fitting(data)),
             color='Darkred', label="predicted\ncrosssection", alpha=0.5, linewidth=2.1)
    ax_1.legend(bbox_to_anchor=(1.05, 1), loc='upper center', borderaxespad=0)



    # Plot b) Chi-squared log contour map


    # Titles and axes for plot b)
    ax_2 = fig.add_subplot(gridspec.GridSpec(3, 1)[1])
    ax_2.set_xlabel(r'$m_{z}$ (GeV)')
    ax_2.set_ylabel(r'$\Gamma_{z}$ (GeV)')
    ax_2.set_title(r"B) Contour plot of $\log$ $\tilde{\chi}^{2}_{red}$ "\
                    "for variable $Z_{0}$ mass and width")
    ax_2.set_xlim([9*curve_fitting(data)[0]/10, 10.5*curve_fitting(data)[0]/10])

    # Log base 1.1 is used on contour plot for optimisation purposes
    contour_plot_1 = ax_2.contourf(*mesh_arrays(contour_data_b),
                                   chi_square_contour(data, contour_data_b[0],
                                                      contour_data_b[1], INTERVALS, True),
                                   INTERVALS, locator=ticker.LogLocator(1.1), cmap='PuRd')
    # Set up colourbar
    cbar = fig.colorbar(contour_plot_1, cax=plt.axes([0.95, .37, 0.04, 0.3]))
    cbar.set_label(r"Low $\tilde{\chi}^{2}_{red}$ $\longrightarrow$ High $\chi^{2}_{red}$")
    cbar.ax.yaxis.set_tick_params(labelright=False)



    # Plot C) Zoomed in chi-squared contour plot


    # Titles and axes for c)
    ax_3 = fig.add_subplot(gridspec.GridSpec(3, 1)[2])
    ax_3.set_xlabel(r'$m_{z}$ (GeV)')
    ax_3.set_ylabel(r'$\Gamma_{z}$ (GeV)')
    ax_3.set_title(r"C) Contour plot of $\chi^{2}$ "\
                    r"with $\sigma_{\chi_{min}^{2}}$ lines")
    ax_3.set_xlim([0.9992*curve_fitting(data)[0], 1.001*curve_fitting(data)[0]])
    ax_3.set_ylim([0.975*curve_fitting(data)[1], 1.01*curve_fitting(data)[1]])

    # Contour plot of chi-squared values at one standard deviation away from minimum.
    contour_plot_2 = ax_3.contour(*mesh_arrays(contour_data_c),
                                  chi_square_contour(data, contour_data_c[0],
                                                     contour_data_c[1], INTERVALS_2, False),
                                  levels=[chi_square(data) + 1.00, chi_square(data) + 2.30,
                                          chi_square(data) + 5.99, chi_square(data) + 9.21],
                                  linewidths=4.5,
                                  colors=("mediumorchid", "deeppink", "palevioletred", "salmon"))

    # Set-up labels and legend for contour lines
    labels = [r"$\chi_{min}^{2}$ + 1.00 (1 $\sigma_{\chi_{min}^{2}}$)",
              r"$\chi_{min}^{2}$ + 2.30 (2 $\sigma_{\chi_{min}^{2}}$)",
              r"$\chi_{min}^{2}$ + 5.99 (3 $\sigma_{\chi_{min}^{2}}$)",
              r"$\chi_{min}^{2}$ + 9.21 (4 $\sigma_{\chi_{min}^{2}}$)"]

    for i, _ in enumerate(labels):
        contour_plot_2.collections[i].set_label(labels[i])
    plt.legend()

    # Get the coordinates for the nearest (s.d. = 1) contour line.
    masses = contour_plot_2.collections[0].get_paths()[0].vertices[:, 0]
    widths = contour_plot_2.collections[0].get_paths()[0].vertices[:, 1]

    # Plot max and min masses and widths of contour lines onto contour plot.
    ax_3.hlines(y=np.amax(widths), xmin=0.9992*curve_fitting(data)[0],
                xmax=0.9999*curve_fitting(data)[0], color='navy',
                ls="--", label=r"max $\Gamma_{z}$ = %s" %np.round(np.amax(widths), 3))
    ax_3.hlines(y=np.amin(widths), xmin=0.9992*curve_fitting(data)[0],\
                xmax=0.9999*curve_fitting(data)[0], color='navy',
                ls=":", label=(r"min $\Gamma_{z}$ = %s" %np.round(np.amin(widths), 3)))
    ax_3.vlines(x=np.amax(masses), ymin=0.975*curve_fitting(data)[1],\
                ymax=0.995*curve_fitting(data)[1], color='teal',
                ls="--", label=r"max $m_{z}$ = %s" %np.round(np.amax(masses), 3))
    ax_3.vlines(x=np.amin(masses), ymin=0.975*curve_fitting(data)[1],\
                ymax=0.995*curve_fitting(data)[1], color='teal',
                ls=':', label=r"min $m_{z}$ = %s" %np.round(np.amin(masses), 3))
    plt.legend(loc='right', bbox_to_anchor=(1.2, 0.5))


    # Finish plot
    plt.subplots_adjust(left=0.1, bottom=0.1, right=0.9, top=0.9, wspace=0.4, hspace=0.8)
    plt.savefig('10621521 Cameron Michie final assignment', format="png", bbox_inches='tight')
    plt.show()

    # Returns upper and lower bounds of mass: (upper mass, lower mass, upper width, lower width)
    return np.amax(masses), np.amin(masses), np.amax(widths), np.amin(widths)


def crosssection(energy, z0_mass, z0_width, z0_partial_width):
    """
    A restatement of equation (4) from assignment script.
    The function inputs energy and calculates predicted values for the
    crosssection.
    Final three arguments are the variables to be minimised, and unlike
    crosssection_fpw(), this includes z0_partial_width.

    Parameters
    ----------
    energy : 1D array
        Zeroth column of data, or x values.
    z0_mass : float
    z0_width : float
    z0_partial_width : float

    Returns
    -------
    crosssection_data : 1D array
        Predicted crosssection values.

    """

    return 12*np.pi * CONVERT_NATURAL_UNITS_TO_NB * energy**2 * z0_partial_width**2 / \
            (z0_mass**2 * ((energy**2 - z0_mass**2)**2  + z0_mass**2 * z0_width**2))


def chi_square(data):
    """
    Iterates through data summing the chi-square values for each data point.

    Parameters
    ----------
    data : array
        array of filtered data in form [x_values, y_values, y_error].

    Returns
    -------
    Chi-square value as float.

    """

    chi_square_array = np.empty([0, 1])

    for i in range(len(data)):

        temp = np.empty([0])

        observation, observation_uncertainty, prediction = \
            data[i, 1], data[i, 2], crosssection(data[:, 0], *curve_fitting(data))[i]

        temp = (observation - prediction)**2 / observation_uncertainty**2

        chi_square_array = np.vstack((chi_square_array, temp))

    return np.sum(chi_square_array)


def chi_square_contour(data, z0_mass, z0_width, intervals, text_not_yet_printed):
    """
    Function iterates through arrays of z0_mass and z0_width, calulating the
    reduced chi-squared at each value.
    Then, an chi_squared array mesh is created with mass varying along the rows and
    width varying along the columns.
    Since this is the function that takes the most computational time, a warning that
    the programme is still running but is not broken is displayed after ~10 seconds.

    Parameters
    ----------
    data : array
        Array of filtered data in form [x_values, y_values, y_error].
    z0_mass : 1D array
        A list of varying mass with "intervals" number of elements.
        Get this list from the first column of get_contour_data().
    z0_width : 1D array
        A list of varying width with "intervals" number of elements.
        Get this list from the first column of get_contour_data().
    text_not_yet_printed : bool
        Turns false once function is run once, so that the text saying
        ~10 seconds has passed does not get printed in console twice.

    Returns
    -------
    Array
        A mesh of reduced chi-squared values, for variable mass in the rows and
        variable width in the columns.

    """
    chi_square_array = np.empty([0, intervals])

    for index in range(intervals):
        chi_square_row = np.empty([0])

        for index_2 in range(intervals):
            array_of_temp = np.empty([0])

            for i in range(len(data)): # Find chi-squared

                while (time.time() - start_time) > 10 and text_not_yet_printed:
                    print("The programme has been running for ~10 seconds. It is not "\
                          "broken, but if you are getting bored and would like the "\
                          "program to run quicker, consider reducing the size of "\
                          "global variables INTERVALS or INTERVALS_2.\n")
                    text_not_yet_printed = False
                    break

                temp = np.empty([])

                observation, observation_uncertainty, prediction = \
                    data[i, 1], data[i, 2], crosssection(data[:, 0], z0_mass[index_2],\
                                                         z0_width[index], Z0_PARTIAL_WIDTH)[i]

                temp = (observation - prediction)**2 / observation_uncertainty**2

                # Construct array of individual chi-squareds before they're summed.
                array_of_temp = np.hstack((array_of_temp, temp))

            # Add element to the end of row.
            chi_square_row = np.hstack((chi_square_row, np.sum(array_of_temp)))

        # Stack row onto array.
        chi_square_array = np.vstack((chi_square_array, chi_square_row))

    # Convert to reduced chi-squared by dividing by degrees of freedom.
    return chi_square_array


def mesh_arrays(data):
    """
    Creates mesh of x and y values for use in contour plotting.

    Parameters
    ----------
    data : array
        Array of filtered data in form [x_values, y_values, ...].

    Returns
    -------
    x_values_mesh : Array
        Mesh of x values.
    y_values_mesh : Array
        Mesh of y values.

    """

    x_values, y_values = data[0], data[1]
    x_values_mesh = np.empty([0, len(x_values)])
    y_values_mesh = np.empty([0, len(y_values)])

    for _ in enumerate(y_values):
        x_values_mesh = np.vstack((x_values_mesh, x_values))

    for _ in enumerate(x_values):
        y_values_mesh = np.vstack((y_values_mesh, y_values))

    y_values_mesh = np.transpose(y_values_mesh)

    return x_values_mesh, y_values_mesh


def curve_fitting(data):
    """
    Minimises z0_mass, z0_width, z0_partial_width with respect to crosssection().
    Uses curve_fit from scipy to do a least-squares fitting of the data. curve_fit operates
    on crosssection(energy, z0_mass, z0_width, z0_partial_width),
    so does a least squares fitting including the partial width of Z0, even though the
    value of this is provided in the assignment script.

    Parameters
    ----------
    data : array
        array of data in form [x_values, y_values, y_error].

    Returns
    -------
    z0_mass: float
        The minimised value for z0_mass.
    z0_width: float
        The minimised value for z0_width.
    z0_partial_width: float
        The minimised value for z0_partial_width

    """

    results = curve_fit(crosssection, data[:, 0], data[:, 1], np.array([
        Z0_MASS_START, Z0_WIDTH_START, Z0_PARTIAL_WIDTH_START]), data[:, 2], maxfev=5000)[0]

    return results[0], results[1], results[2]


def curve_fitting_uncertainties(data):
    """
    Finds uncertainty on minimised values of z0_mass, z0_width, z0_partial_width.
    Uses curve_fit from scipy to find the uncertainty on a least-squares fitting of the data.
    curve_fit operates on crosssection(energy, z0_mass, z0_width, z0_partial_width).
    This least squares fitting including the partial width of Z0, even though the
    value of this is provided in the assignment script (for comparison purposes).

    Parameters
    ----------
    data : array
        array of data in form [x_values, y_values, y_error].

    Returns
    -------
    Array of uncertainties in form
    [z0_mass_uncertainty, z0_width_uncertainty, z0_partial_width_uncertainty]

    """

    variances = curve_fit(crosssection, data[:, 0], data[:, 1], np.array([
        Z0_MASS_START, Z0_WIDTH_START, Z0_PARTIAL_WIDTH_START]), data[:, 2], maxfev=2000)[1]

    return  np.sqrt(np.diag(variances))


def crosssection_fpw(energy, z0_mass, z0_width):
    """
    A restatement of equation (4) from assignment script.
    The function inputs energy and calculates predicted values for the
    crosssection.
    Final two arguments are the variables to be minimised, and unlike
    crosssection(), this does not includes z0_partial_width, instead using
    the global variable Z0_PARTIAL_WIDTH.

    Parameters
    ----------
    data : array
        array of data in form [x_values, y_values, y_error].

    Returns
    -------
    z0_mass: float
        The minimised value for z0_mass.
    z0_width: float
        The minimised value for z0_width.
    z0_partial_width: float
        The minimised value for z0_partial_width

    """

    return 12*np.pi * CONVERT_NATURAL_UNITS_TO_NB * energy**2 * Z0_PARTIAL_WIDTH**2 / \
            (z0_mass**2 * ((energy**2 - z0_mass**2)**2 + z0_mass**2 * z0_width**2))

def curve_fitting_fpw(data):
    """
    Minimises z0_mass, z0_width, z0_partial_width with respect to crosssection_fpw().

    "fpw" refers to "fixed partial width" because in this function partial width is held
    constant at the quoted value from assignment script.

    Uses curve_fit from scipy to do a least-squares fitting of the data.
    curve_fit operates on crosssection_fpw(energy, z0_mass, z0_width).

    Parameters
    ----------
    data : array
        array of data in form [x_values, y_values, y_error].

    Returns
    -------
    z0_mass: float
        The minimised value for z0_mass.
    z0_width: float
        The minimised value for z0_width.

    """

    results = curve_fit(crosssection_fpw, data[:, 0], data[:, 1], np.array([
        Z0_MASS_START, Z0_WIDTH_START]), data[:, 2], maxfev=5000, method='dogbox')[0]

    return results[0], results[1]

def curve_fitting_fpw_uncertainties(data):
    """
    Finds uncertainty on minimised values of z0_mass and z0_width at constant
    partial width. "fpw" refers to "fixed partial width" because in this function
    partial width is held constant at the quoted value from assignment script.

    Uses curve_fit from scipy to find the uncertainty on a least-squares fitting of the data.
    curve_fit operates on crosssection(energy, z0_mass, z0_width).

    Parameters
    ----------
    data : array
        array of data in form [x_values, y_values, y_error].

    Returns
    -------
    Array of uncertainties in form
    [z0_mass_uncertainty, z0_width_uncertainty]

    """
    variances = curve_fit(crosssection_fpw, data[:, 0], data[:, 1], np.array([
        Z0_MASS_START, Z0_WIDTH_START]), data[:, 2], maxfev=2000, method='dogbox')[1]
    return np.sqrt(np.diag(variances))

def get_contour_uncertainties(contour_z0_values):
    """
    Returns the contour determined uncertainties given the max and min values
    of the mass and width at one standard devation away from the minimised
    chi-squared value.

    Parameters
    ----------
    contour_z0_values : tuple
        Has form (max_mass, min_mass, max_width, min_width).

    Returns
    -------
    Uncertainties : tuple
        Has form (mass_contour_uncertainty, width_contour_uncertainty).

    """

    return ((contour_z0_values[0] - contour_z0_values[1])/2,\
            (contour_z0_values[2]-contour_z0_values[3])/2)


def main():
    """
    Main bulk of code.
    Firstly, functions are run which read, filter and combine the data, which is then plotted.
    After this, functions which find the LSF results are run, and the results are printed.
    No arguments and returns nothing.
    """
    # Read in data
    data = combine_data(FILE1, FILE2)

    # Plot data and declare minimised values found by chi-squared contour fit.
    contour_z0_values = plot_data(data)

    # Get uncertainties from these contour values
    contour_uncertainties = get_contour_uncertainties(contour_z0_values)

    # Get results for fixed and variable partial width.
    z0_mass, z0_width, z0_partial_width = np.absolute(curve_fitting(data))
    z0_mass_uncert, z0_width_uncert, z0_partial_width_uncert = curve_fitting_uncertainties(data)
    z0_mass_fpw, z0_width_fpw = np.absolute(curve_fitting_fpw(data))
    z0_mass_fpw_uncert, z0_width_fpw_uncert = np.absolute(curve_fitting_fpw_uncertainties(data))

    # Least squares fitting results
    print("The least squares fitting results:\n")

    # Print results for variable partial width.
    print("For variable partial width:")
    print("Z0 mass  = {:.4g} ± {:.2g} GeV/c^2"
          .format(np.absolute(z0_mass), z0_mass_uncert))
    print("Z0 width = {:.4g} ± {:.2g} GeV".format(z0_width, z0_width_uncert))
    print("Z0 partial width = {:.4g} ± {:.2g} MeV.\nThis compares to the quoted value "\
          "of 83.91 MeV and differs by {:.3g}% or {:.3g} standard deviations."\
          .format(z0_partial_width*10**3, z0_partial_width_uncert,
                  np.absolute(100 - z0_partial_width*10**3 *100 / 83.91),\
                      np.absolute(z0_partial_width*10**3 - 83.91)/z0_partial_width_uncert))
    print("Tau z = {:.3g} seconds".format(1.0545e-34 / (z0_width*1.602177e-10)))

    # Print chi-squared.
    print("\nChi square is {:.2f}".format(chi_square(data)))
    print("Reduced chi square is {:.3f}".format(chi_square(data)/(len(data[:, 0]) - 2)))
    print("Note: Chi-square calculated for variable partial width.")

    # Print results for fixed partial width.
    print("\nFor fixed partial width, the least squares fitting yields:")
    print("Z0 mass  = {:.4g} ± {:.2g} GeV/c^2"
          .format(z0_mass_fpw, z0_mass_fpw_uncert))
    print("Z0 width = {:.4g} ± {:.2g} GeV".format(z0_width_fpw, z0_width_fpw_uncert))
    print("Tau z = {:.3g} seconds".format(1.0545e-34 / (z0_width_fpw * 1.602177e-10)))

    # Print results from contour plot
    print("\nThe results from the minimised chi-square contour plot:\n"\
          "Z0 mass = {:.4g} ± {:.1g} GeV/c^2\n"\
          "Z0 width = {:.4g} ± {:.1g} GeV".format(
              (contour_z0_values[0] + contour_z0_values[1])/2, contour_uncertainties[0],
              (contour_z0_values[2] + contour_z0_values[3])/2, contour_uncertainties[1]))


while filecheck(FILE1) and filecheck(FILE2):

    start_time = time.time()
    main()
    print("\nCode completed in {:.3g}s seconds".format(time.time() - start_time))
    break

else:

    print("Sadly, at least one of the files was not found.")
